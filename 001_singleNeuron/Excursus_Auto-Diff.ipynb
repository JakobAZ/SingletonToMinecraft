{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Differentiation\n",
    "\n",
    "## Short explanation\n",
    "The idea behind auto-derivation is to construct an evaluation tree for some highly nested function and deriving this function by recursively \"evaluating\" (where the way to evaluate it is to derive the subtrees) the tree giving the recusion the basic rules of derivation as anchors. By 'basic rules' I mean these: \n",
    "<ol>\n",
    "<li> +: the derivative of any sum is the sum of the derivatives\n",
    "<li> *: the derivative of any product is the derivative of the first factor times the second plus the first factor times the derivative of the second.\n",
    "<li> power: $\\frac{ \\partial x^k}{\\partial x} = k \\cdot x^{k-1}$\n",
    "<li> ...\n",
    "</ol>\n",
    "\n",
    "## Long excurse\n",
    "\n",
    "One of the reasons, machine learning could get this powerful is that there is a way to automatically compute very complicated differentials. While it is relatively straightforward to calculate the derivative of the cost-function with respect to the parameters of a single-neuron 'net', calculating the gradient of the weights for a neuron that is not in the output-layer is a bit more involved: the loss is a function of the output-layer-neurons's inputs, (some of) which are functions of the weights in question. The chain of dependency is of course getting longer the deeper and more interconnected a net becomes.\n",
    "Having to derive all of the gradients ourselves for a big, dense neural net would take forever. \n",
    "\n",
    "That is where automatic differentiation comes in. There is a relatively small amount of mathematical operations (+, -, *, /, power, exp, sin, cos) that are the building blocks of any function. Knowing the chain rule, we can calculate arbitrarily nested derivatives. \n",
    "\n",
    "Let's call any neuron $\\mathcal{N}_{i, j}$ where $i$ is an index denoting its layer (output layer is 0, second-to-last 1, etc.) and  $j$ is some index that differentiates the neuron from other ones in the given layer. Let's call the weights of $\\mathcal{N}_{i, j}$ $w_{i, j}$, the weighted sum of inputs of that neuron $z_{i,j}$ and its activation function $act_{i,j}$.\n",
    "Returning to our example of deriving some loss function with respect to the weights of a neuron in the second-to-last layer we get \n",
    "$$ \\frac{ \\partial \\mathcal{L}}{\\partial w_{1,j_0}} = \\frac{ \\partial \\mathcal{L}}{\\partial act_{0,0}} \\cdot \\frac{ \\partial act_{0,0}}{\\partial z_{0,0}} \\cdot \\frac{ \\partial z_{0, 0}}{\\partial act_{1,j_0}}\\cdot \\frac{\\partial act_{1,j_0}}{ \\partial w_{1,j_0}}\n",
    "$$\n",
    "\n",
    "Automatic derivation now decomposes these derivatives into its base-parts, fills in the derivations and evaluates. For example, because $z_{0,0}$ is a sum, autodiff would replace this with the sum of the derivatives. This will be a sum of derivatives of activation functions which can be further derived or, if they are independent of the weights we are interested in, can be set to 0. \n",
    "\n",
    "Let us code an algorithm that illustrates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "f = lambda x: x+1\n",
    "f_vec = np.vectorize(f)\n",
    "\n",
    "f_vec(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 31, 31, 31])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class expression: \n",
    "    def __init__(self, operator, left, right) -> None:\n",
    "        self.operator = operator\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def evaluate(self):\n",
    "        return eval(self.left+self.operator+self.right)\n",
    "arr = expression(\"+\", \"23\", \"8\"), expression(\"+\", \"23\", \"8\"), expression(\"+\", \"23\", \"8\"), expression(\"+\", \"23\", \"8\")\n",
    "\n",
    "getvals= np.vectorize(expression.evaluate)\n",
    "getvals(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "class count:\n",
    "    classvar = 0\n",
    "    def __init__(self) -> None:\n",
    "        self.ID = count.classvar \n",
    "        count.classvar += 1\n",
    "    giveID = lambda self: self.ID\n",
    "    give_arr = np.vectorize(giveID)\n",
    "\n",
    "a, b, c = count(), count(), count()\n",
    "\n",
    "for i in a,b,c: \n",
    "    print(count.give_arr([a,b,c]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ Author: Jakob Abou Zeid\n",
    "# @ email: jakob.abouzeid@gmail.com\n",
    "\n",
    "\n",
    "import typing\n",
    "from collections.abc import Callable\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    next_neuron_ID = 0\n",
    "    def __init__(self, in_list: typing.List['Neuron'], activation: Callable) -> None:\n",
    "        self.activation = activation\n",
    "        self.parameters = np.random.rand(in_list.__len__()+1,1)\n",
    "        self.in_list = in_list\n",
    "        self.out_value = None\n",
    "        self.neuron_ID = Neuron.next_neuron_ID\n",
    "        Neuron.next_neuron_ID +=1\n",
    "\n",
    "    def give_output(self: 'Neuron'):\n",
    "        if self.out_value is None:\n",
    "            self.activate()\n",
    "        return self.out_value\n",
    "\n",
    "    def get_inputs(self):\n",
    "        # We reshape the input we get rather than the output we deliver. \n",
    "        # This ensures that the input layer can be initialized with rows or columns or 1-d arrays. Handy!\n",
    "        return np.hstack([in_neuron.give_output().reshape(-1,1) for in_neuron in self.in_list])\n",
    "\n",
    "    def activate(self):\n",
    "        inputs = self.get_inputs()\n",
    "        z = np.hstack([inputs, np.ones((inputs.shape[0],1))]).dot(self.parameters)\n",
    "        z = np.sum(z,axis=1)\n",
    "        self.out_value = self.activation(z)\n",
    "        return (self.out_value)\n",
    "\n",
    "    def reset(self):\n",
    "        self.out_value = None\n",
    "        for child in self.in_list:\n",
    "            child.reset()\n",
    "    \n",
    "\n",
    "\n",
    "test = [Neuron([], lambda x:x), Neuron([], lambda x:x)]\n",
    "for neuron in test:\n",
    "    neuron.out_value = np.arange(10).reshape(-1)\n",
    "\n",
    "parent = Neuron(test, lambda x:x)\n",
    "parent.parameters = 1\n",
    "parent.activate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None or \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def print_all_IDs(root: Neuron):\n",
    "    print(root.neuron_ID)\n",
    "    for neuron in root.in_list:\n",
    "        print_all_IDs(neuron)\n",
    "\n",
    "print_all_IDs(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an input layer of 2 neurons:\n",
    "num_input_neurons = 2\n",
    "in_layer = [Neuron([], lambda x:x) for i in range(num_input_neurons)]\n",
    "\n",
    "for neuron, val in zip(in_layer, [2,3]):\n",
    "    neuron.out_value = val\n",
    "\n",
    "out_neuron = Neuron(in_layer, lambda x:x)\n",
    "out_neuron.parameters = 1\n",
    "\n",
    "out_neuron.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.arange(4).reshape(-1,1)\n",
    "arr2 = np.arange(4).reshape(-1,1)\n",
    "\n",
    "np.hstack([arr1, arr2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriving e wrt c (Value 6):  grad = 1, coeff= 1\n",
      "deriving c wrt a (Value 2):  grad = 1, coeff= 3\n",
      "deriving c wrt b (Value 3):  grad = 1, coeff= 2\n",
      "deriving e wrt d (Value 0.9092974268256817):  grad = 1, coeff= 1\n",
      "deriving d wrt a (Value 2):  grad = 1, coeff= -0.4161468365471424\n",
      "f = 6.909297426825682\n",
      "∂f/∂x = 2.5838531634528574 [∂e/∂a]\n",
      "∂f/∂y = 2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Var:\n",
    "    nextID = 'a'\n",
    "    def __init__(self, value, children=None, coeff = None):\n",
    "        self.value = value\n",
    "        self.children = children or []\n",
    "        self.grad = 0\n",
    "        self.coeff = coeff\n",
    "        self.ID = Var.nextID\n",
    "        Var.nextID = chr(ord(Var.nextID)+1)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Var(self.value + other.value, [self, other], dict([(self.ID, 1), (other.ID, 1)]))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Var(self.value * other.value, [self, other],  dict([(self.ID, other.value), (other.ID, self.value)]))\n",
    "\n",
    "    def sin(self):\n",
    "        return Var(math.sin(self.value),[self], {self.ID: math.cos(self.value)})\n",
    "\n",
    "    def calc_grad(self, grad=1):\n",
    "        self.grad += grad\n",
    "        for child in self.children:\n",
    "            coeff = self.coeff[child.ID]\n",
    "            print(f\"deriving {self.ID} wrt {child.ID} (Value {child.value}): \", f\"grad = {self.grad}, coeff= {coeff}\")\n",
    "            child.calc_grad(grad * coeff)\n",
    "\n",
    "# Example: f(x, y) = x * y + sin(x)\n",
    "x = Var(2)\n",
    "y = Var(3)\n",
    "f = x * y \n",
    "f = f + x.sin()\n",
    "\n",
    "# Calculation of partial derivatives\n",
    "f.calc_grad()\n",
    "\n",
    "print(\"f =\", f.value)\n",
    "print(\"∂f/∂x =\", x.grad, f\"[∂{f.ID}/∂{x.ID}]\")\n",
    "print(\"∂f/∂y =\", y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inOrder(root: Var, indents = 0):\n",
    "    print(\"\\t\"*indents, root.ID, \":\", root.value)\n",
    "    for child in root.children:\n",
    "        inOrder(child, indents+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " e : 6.909297426825682\n",
      "\t c : 6\n",
      "\t\t a : 2\n",
      "\t\t b : 3\n",
      "\t d : 0.9092974268256817\n",
      "\t\t a : 2\n"
     ]
    }
   ],
   "source": [
    "inOrder(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Author: Jakob Abou Zeid\n",
    "# @ email: jakob.abouzeid@gmail.com\n",
    "import typing\n",
    "from collections.abc import Callable\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    next_neuron_ID = 0\n",
    "    def __init__(self, in_list: typing.List['Neuron'], activation: Callable) -> None:\n",
    "        self.activation = activation\n",
    "        self.parameters = np.random.rand(in_list.__len__()+1,1)\n",
    "        self.in_list = in_list\n",
    "        self.out_value = None\n",
    "        self.neuron_ID = Neuron.next_neuron_ID\n",
    "        self.parents = []\n",
    "        Neuron.next_neuron_ID +=1\n",
    "\n",
    "    def give_output(self: 'Neuron') -> float:\n",
    "        if self.out_value is None:\n",
    "            self.activate()\n",
    "        return self.out_value\n",
    "\n",
    "    def get_inputs(self):\n",
    "        # We reshape the input we get rather than the output we deliver. \n",
    "        # This ensures that the input layer can be initialized with rows or columns or 1-d arrays. Handy!\n",
    "        return np.hstack([in_neuron.give_output().reshape(-1,1) for in_neuron in self.in_list])\n",
    "\n",
    "    def activate(self):\n",
    "        inputs = self.get_inputs()\n",
    "        z = np.hstack([inputs, np.ones((inputs.shape[0],1))]).dot(self.parameters)\n",
    "        z = np.sum(z,axis=1)\n",
    "        self.out_value = self.activation(z)\n",
    "        return (self.out_value)\n",
    "\n",
    "    def reset(self):\n",
    "        self.out_value = None\n",
    "        for child in self.in_list:\n",
    "            child.reset()\n",
    "    \n",
    "    def set_children_backlink(self):\n",
    "        for child in self.in_list:\n",
    "            child.parents.append(self)\n",
    "            child.set_children_backlink()\n",
    "        \n",
    "\n",
    "class Net:\n",
    "    def __init__(self, root: Neuron):\n",
    "        self.root = root\n",
    "        root.set_children_backlink()\n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "test = [Neuron([], None), Neuron([], None)]\n",
    "for neuron in test:\n",
    "    neuron.out_value = np.arange(10).reshape(-1)\n",
    "\n",
    "parent = Neuron(test, lambda x:x, [lambda : 1, lambda : 1])\n",
    "parent.parameters = 1\n",
    "parent.activate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".00venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dae194c3054657bea38ca3a00ed0cd8c059a67fdec2609f3490360210ec38667"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
